### Materials
* Lecture slides - [__part1__](https://github.com/yandexdataschool/nlp_course/blob/d7df7ebe30c8bcf662ec246e36ac0ed8115236c2/resources/slides/nlp19_04_seq2seq_attention.pdf) [__part2__](https://drive.google.com/file/d/1ueMYgH3qhsjn3X6K_mcnMvYjjh9Zvl01/view) (by Lena Voita)
* Our videos (russian): [lecture1](https://yadi.sk/i/CX1M4cKnTuC3kg), [lecture2](https://yadi.sk/i/81nP3AcDIrBE5g) [seminar](https://yadi.sk/i/b_64Rs1anbTx9A)
* Interactive classes: [__attention__](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html), [__transfer learning__](https://lena-voita.github.io/nlp_course/transfer_learning.html)
* Stanford lecture on attention and transformers (english) - [video](https://www.youtube.com/watch?v=5vcj8kSwBCY)
* Alternative CMU lectures - [seq2seq](https://www.youtube.com/watch?v=aHkgjfKvIhk&list=PL8PYTP1V4I8Ba7-rY4FoB4-jfuJ7VDKEE&index=20) and [attention](https://www.youtube.com/watch?v=ullLRKZ99qQ&index=21&list=PL8PYTP1V4I8Ba7-rY4FoB4-jfuJ7VDKEE)

### Practice
This time there are two parts:

1. seminar: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yandexdataschool/practical_dl/blob/fall24/week07_attention/seminar.ipynb)

2. homework: see `homework.ipynb`

Please submit both to the anytask.